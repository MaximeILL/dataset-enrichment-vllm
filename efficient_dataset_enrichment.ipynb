{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1bVDPsENZDu"
   },
   "source": [
    "# Efficient Dataset Enrichment Using vLLM\n",
    "\n",
    "This notebook implements an efficient batch inference pipeline using vLLM for dataset enrichment. Key features include :\n",
    "\n",
    "*   Parallel inference with Qwen2-VL model distributed across 4 GPUs  \n",
    "*   Large-scale processing with batches of 120 samples\n",
    "*   Custom vision-language prompting system  \n",
    "*   Automated JSON output logging with error handling\n",
    "*   Multi-modal processing pipeline (images and text)        \n",
    "    \n",
    "*To use this notebook, you need to:*\n",
    "\n",
    "1.   Choose your model path (default: local Qwen2-VL)\n",
    "2.   Select your dataset from Hugging Face hub  \n",
    "3.   Define your custom prompt based on your use case  \n",
    "\n",
    "*Note:*  While the overall code structure is reusable, **the prompt format must be adapted to your specific model**. The current implementation shows Qwen2-VL's format - please refer to vLLM documentation for the correct prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXBDgWEQbZCX"
   },
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    \"\"\" Loads a local model distributed across 4 GPUs for parallel inference\"\"\"\n",
    "    return LLM(\n",
    "        # loading local model, here qwen2-vl\n",
    "        model='./model/qwen2',\n",
    "        tensor_parallel_size=4,\n",
    "        max_num_seqs=5,\n",
    "    )\n",
    "\n",
    "def ensure_rgb_image(image):\n",
    "    \"\"\" Converts images to RGB format using PIL \"\"\"\n",
    "    try:\n",
    "        if image.mode != 'RGB':\n",
    "            return image.convert('RGB')\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2-FednCYGcL"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"your/dataset\")\n",
    "\n",
    "# Define prompt system\n",
    "prompt = \"\"\" Enter prompt \"\"\"\n",
    "\n",
    "train_data = dataset['train']\n",
    "llm = setup_model()\n",
    "\n",
    "# initialize output JSON file with timestamp\n",
    "output_file = f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
    "\n",
    "batch_size = 120\n",
    "sampling_params = SamplingParams(temperature=0.2, max_tokens=512)\n",
    "\n",
    "# Process batches and save results\n",
    "with open(output_file, 'w') as f:\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size)):\n",
    "        batch = dataset['train'][i:i + batch_size]\n",
    "\n",
    "        # inputs with Qwen2-VL specific prompt format\n",
    "        inputs = [{\n",
    "            \"prompt\": \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>\" + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "            \"multi_modal_data\": {\n",
    "                     \"image\": ensure_rgb_image(img)\n",
    "            }\n",
    "        } for img in batch['image']]\n",
    "\n",
    "        # Generate and save results\n",
    "        try:\n",
    "            outputs = llm.generate(inputs, sampling_params)\n",
    "            for j, out in enumerate(outputs):\n",
    "                json.dump({\"idx\": i+j, \"text\": out.outputs[0].text}, f)\n",
    "                f.write('\\n')\n",
    "            f.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {i//batch_size} failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7Huec5sX4sc"
   },
   "outputs": [],
   "source": [
    "elements = []\n",
    "with open(output_file, 'r') as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line)\n",
    "        generated_element = result['text']\n",
    "        elements.append(generated_element)\n",
    "\n",
    "# Verify data integrity\n",
    "assert len(elements) == len(dataset['train']), f\"Number of generated elements ({len(elements)}) doesn't match dataset size ({len(dataset['train'])})\"\n",
    "\n",
    "#Create new dataset with generated data column\n",
    "new_dataset_dict = DatasetDict()\n",
    "\n",
    "for split in dataset.keys():\n",
    "    split_dict = dataset[split].to_dict()\n",
    "    if split == 'train':\n",
    "        split_dict['column_generated'] = elements\n",
    "\n",
    "    new_dataset_dict[split] = Dataset.from_dict(split_dict)\n",
    "\n",
    "print(\"New DatasetDict structure:\")\n",
    "print(new_dataset_dict)\n",
    "print(\"\\nNumber of examples per split:\")\n",
    "\n",
    "for split in new_dataset_dict:\n",
    "    print(f\"{split}: {len(new_dataset_dict[split])} examples\")\n",
    "print(\"\\nColumns in train split:\", new_dataset_dict['train'].column_names)\n",
    "\n",
    "# Save your new enriched dataset\n",
    "new_dataset_dict.save_to_disk(\"dataset_with_generated_data\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
